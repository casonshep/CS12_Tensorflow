{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: TensorFlow Basics (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Tensors\n",
    "If you want to review any of this material, look over https://docs.scipy.org/doc/numpy-1.15.0/user/quickstart.html and https://www.tensorflow.org/guide/tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Tensor values, shapes, rank, and axes\n",
    "Make tensor values by hand (e.g. `x = np.array([[1, 2, 3], [4, 5, 6]])`) of the following shapes:\n",
    " * a: (2, 2)\n",
    " * b: (3)\n",
    " * c: (3, 1)\n",
    " * d: (1, 3)\n",
    " * e: ()\n",
    " * f: (1)\n",
    " * g: (2, 2, 2)\n",
    " * h: (2, 3, 1, 2)\n",
    " \n",
    " For each, put its tensor rank and total number of elements in a comment.\n",
    " Yes, this is pretty boring, but it's also short and it's really important to understand what tensors of different shapes look like and how shapes, rank, and axes interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], [3, 4]])                     # rank: 2, elements: 4\n",
    "b = np.array([1, 2, 3])                            # rank: 1, elements: 3\n",
    "c = np.array([[1], [2], [3]])                      # rank: 2, elements: 3\n",
    "d = np.array([[1, 2, 3]])                          # rank: 2, elements: 3\n",
    "e = np.array(7)                                    # rank: 0, elements: 1\n",
    "f = np.array([42])                                 # rank: 1, elements: 1\n",
    "g = np.array([[[1, 2], [3, 4]], \n",
    "              [[5, 6], [7, 8]]])                   # rank: 3, elements: 8\n",
    "h = np.array([[[[1, 2]], [[3, 4]], [[5, 6]]],\n",
    "              [[[7, 8]], [[9, 10]], [[11, 12]]]])  # rank: 4, elements: 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Slices and reductions\n",
    "Use slicing or `tf.reduce_mean`, `tf.reduce_sum`, and `tf.reduce_any` on the tensors defined below to print:\n",
    " * The (1-2-3)-st element of `a`\n",
    " * The first column of `b`\n",
    " * The shape-(2, 3, 2) tensor obtained by selecting the second and third elements of the third axis of `a`\n",
    " * The sum of all values in `b`\n",
    " * The 2-vector containing means of each row of `b` \n",
    " * The (1, 3) tensor containing, for each column in `c`, whether that column contains any `True` values\n",
    " \n",
    "Each statement should take the form \n",
    "```\n",
    "tf.print(something[...])\n",
    "```\n",
    "or \n",
    "```\n",
    "tf.print(tf.reduce_something(...))\n",
    "```\n",
    "Follow each with a comment stating the shape of the output.\n",
    "For a rank-2 tensor, the first index specifies row and the second specifies column.\n",
    "Make sure to pay attention to the `axis` and `keepdims` arguments of the `reduce` functions.\n",
    " \n",
    " \n",
    "For this problem, I'll set up the name scope, but for all future problems you'll need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.ones((2, 3, 4))) # Tensor of ones with shape (2, 3, 4)\n",
    "b = tf.constant([[1., 2.], \n",
    "                 [3., 4.]]) # Tensor of the matrix [1 2; 3 4] with shape (2, 2)\n",
    "c = tf.constant([[True, True, False],\n",
    "                 [False, True, False]]) # Binary tensor with shape (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]]\n",
      "b: [[1. 2.]\n",
      " [3. 4.]]\n",
      "c: [[ True  True False]\n",
      " [False  True False]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1 3]\n",
      "[[[1 1]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [1 1]]]\n",
      "10\n",
      "[1.5 3.5]\n",
      "[[1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('slices_and_reductions'):\n",
    "    tf.print(a[0, 1, 2])  \n",
    "    # outputs the (1-2-3)-st element (i.e. a[0][1][2])\n",
    "\n",
    "    tf.print(b[:, 0])  \n",
    "    # shape: (2,), first column of b\n",
    "\n",
    "    tf.print(a[:, :, 1:3])  \n",
    "    # shape: (2, 3, 2), slice 2nd and 3rd elements along the last axis\n",
    "\n",
    "    tf.print(tf.reduce_sum(b))  \n",
    "    # sum of all values in b\n",
    "\n",
    "    tf.print(tf.reduce_mean(b, axis=1))  \n",
    "    # shape: (2,), mean of each row\n",
    "\n",
    "    tf.print(tf.reduce_any(c, axis=0, keepdims=True))  \n",
    "    # shape: (1, 3), whether each column has any True value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Transposition and reshaping\n",
    "Use `tf.transpose` to print:\n",
    " * `b` with its rows and columns swapped\n",
    " * `a` with its second and third axes swapped; comment its shape\n",
    " \n",
    "Use `tf.reshape` to print:\n",
    " * The values of `b` in a tensor with shape (1, 4)\n",
    " * The values of `b` in a tensor with shape (4, 1)\n",
    " \n",
    "Do this all inside the name scope \"transposition_and_reshaping\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 4]]\n",
      "[[[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n",
      "[[1 2 3 4]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('transposition_and_reshaping'):\n",
    "    tf.print(tf.transpose(b))  \n",
    "    # shape: (2, 2)\n",
    "\n",
    "    tf.print(tf.transpose(a, perm=[0, 2, 1]))  \n",
    "    # shape: (2, 4, 3), a with second and third axes swapped\n",
    "\n",
    "    tf.print(tf.reshape(b, (1, 4)))  \n",
    "    # shape: (1, 4)\n",
    "\n",
    "    tf.print(tf.reshape(b, (4, 1)))  \n",
    "    # shape: (4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Computing with Operations and Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: The dot product (as a sum of scalar products)\n",
    "Write a function `dot_sum()` that takes in two rank-1 tensors `a` and `b` of equal shape and returns a tensor that holds their dot product, $$\\text{result} = a \\cdot b = \\sum_{i = 1}^{\\dim{a}} a_i \\cdot b_i $$\n",
    "\n",
    "The computation should first multiply the elements in $a$ and $b$ into a vector $a \\odot b$ (the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of $a$ and $b$), then sum across the vector to produce a scalar. \n",
    "Your implementation should be _vectorized_: it should not explicitly use the shape of an input tensor or do any looping.\n",
    "The tensor output by your function must be rank-0.\n",
    "\n",
    "The entire computation should use the name scope \"dot_sum\" and the tensor you return should have the name \"result\".\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * `tf.multiply` (or equivalently, the binary operation *)\n",
    " * `tf.reduce_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_sum(a, b):\n",
    "    '''\n",
    "    Given rank-1 tensors a and b with equal shapes, return the dot product \n",
    "    of a and b as a rank-0 tensor computed via Hadamard product.\n",
    "    '''\n",
    "    with tf.name_scope('dot_sum'):\n",
    "        hadamard_product = tf.multiply(a, b)\n",
    "        \n",
    "        result = tf.reduce_sum(hadamard_product, name=\"result\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: The dot product (as matrix multiplication)\n",
    "Write a function `dot_multiply()` that takes in two rank-1 tensors `a` and `b` of equal shape and returns a tensor that holds their dot product, $$\\text{result} = a \\cdot b = a^T b $$\n",
    "\n",
    "The computation should use `tf.matmul` to perform the multiplication, which expects that your input tensors have rank of at least two (they should be matrices, not vectors).\n",
    "Since your input vectors are rank-1, this means you'll need to use `tf.expand_dims` with `axis=-1` to add a \"dummy dimension\".\n",
    "This is a subtle but important point: your vectors start with shape [n], but matrix multiplication is only defined for matrices with shapes [1, n] and [n, 1].\n",
    "Depending on how you do it, you will probably get a rank-2 tensor with a shape like [1, 1].\n",
    "You must return a rank-0 tensor, so use `tf.squeeze` to eliminate dummy dimensions.\n",
    "\n",
    "The entire computation should use the name scope \"dot_multiply\" and the tensor you return should have the name \"result\".\n",
    "This will not collide with the previous \"result\" tensor because of name scoping.\n",
    "(If it did, it would be renamed to \"result_0\" in the graph)\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * `tf.matmul`\n",
    " * `tf.transpose`\n",
    " * `tf.expand_dims`\n",
    " * `tf.squeeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_multiply(a, b):\n",
    "    '''\n",
    "    Given rank-1 tensors a and b with equal shapes, return the dot product \n",
    "    of a and b as a rank-0 tensor computed via matrix multiplication.\n",
    "    '''\n",
    "    with tf.name_scope('dot_multiply'):\n",
    "        a_exp = tf.expand_dims(a, axis=-1)  # Shape: [n, 1]\n",
    "        b_exp = tf.expand_dims(b, axis=0)  # Shape: [1, n]\n",
    "\n",
    "        result= tf.matmul(a_exp, b_exp)\n",
    "\n",
    "        # (rank-0 tensor)\n",
    "        result = tf.squeeze(result, name=\"result\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: A single ReLU unit\n",
    "The \"default\" activation function for modern neural networks is the [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) (or \"ReLU\"):\n",
    "$$ \\text{relu}(x) = max(0, x). $$\n",
    "\n",
    "In a neural network using ReLU activation, a single unit with $n$ inputs has parameters $w$ (an $n$-vector of weights) and $b$ (a scalar).\n",
    "It computes the function\n",
    "$$ f(x; w, b) = \\text{relu}(w \\cdot x + b). $$\n",
    "\n",
    "Using either `dot_sum` or `dot_multiply`, add these tensors and operations to the default graph:\n",
    "$$\n",
    "\\begin{align}\n",
    "&x: \\space \\text{placeholder} \\\\\n",
    "&w = \\begin{bmatrix}2 & 0.5 & -1\\end{bmatrix} \\\\\n",
    "&b = 0.3 \\\\\n",
    "&\\text{state} = w \\cdot x + b \\\\\n",
    "&\\text{activation} = \\max(\\text{state}, 0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "`x` should have shape [3] and dtype `tf.float32`, and all tensors should be named, under the name scope \"ReLU\".\n",
    "This includes the tensors created through your dot product function, but do not change your implementation to add to the name! Then wrap all of this in a function called `relu` that takes in one argument to initialize the `tf.Variable` `x` object, then returns `activation`.\n",
    "\n",
    "Then, print `relu` for:\n",
    " * $x = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} -1 & 2 & 0 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * tf.constant\n",
    " * tf.Variable\n",
    " * tf.add\n",
    " * tf.maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x_init):\n",
    "    with tf.name_scope(\"ReLU\"):\n",
    "        # single unit\n",
    "        w = tf.constant([2.0, 0.5, -1.0], dtype=tf.float32, name=\"w\")\n",
    "        b = tf.constant(0.3, dtype=tf.float32, name=\"b\") \n",
    "        \n",
    "        # tf.Variable for x with the given initializer\n",
    "        x = tf.Variable(x_init, dtype=tf.float32, name=\"x\")\n",
    "        \n",
    "        state = dot_multiply(w, x) + b\n",
    "        \n",
    "        # Apply ReLU activation: max(state, 0)\n",
    "        activation = tf.maximum(state, 0, name=\"activation\")\n",
    "        \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant([1.0, 1.0, 1.0], dtype=tf.float32)\n",
    "x2 = tf.constant([-1.0, 2.0, 0.0], dtype=tf.float32)\n",
    "x3 = tf.constant([1.0, 0.0, 0.0], dtype=tf.float32)\n",
    "x4 = tf.constant([0.0, 0.0, 0.0], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU output for x = [1, 1, 1]:\n",
      " [[2.3 2.3 2.3]\n",
      " [0.8 0.8 0.8]\n",
      " [0.  0.  0. ]]\n",
      "ReLU output for x = [-1, 2, 0]:\n",
      " [[0.  4.3 0.3]\n",
      " [0.  1.3 0.3]\n",
      " [1.3 0.  0.3]]\n",
      "ReLU output for x = [1, 0, 0]:\n",
      " [[2.3 0.3 0.3]\n",
      " [0.8 0.3 0.3]\n",
      " [0.  0.3 0.3]]\n",
      "ReLU output for x = [0, 0, 0]:\n",
      " [[0.3 0.3 0.3]\n",
      " [0.3 0.3 0.3]\n",
      " [0.3 0.3 0.3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"ReLU output for x = [1, 1, 1]:\\n\", relu(x1).numpy())\n",
    "print(\"ReLU output for x = [-1, 2, 0]:\\n\", relu(x2).numpy())\n",
    "print(\"ReLU output for x = [1, 0, 0]:\\n\", relu(x3).numpy())\n",
    "print(\"ReLU output for x = [0, 0, 0]:\\n\", relu(x4).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside on activation functions\n",
    "\n",
    "One way to derive feedforward neural networks is to begin by saying \"I'd like to do a simple (linear) transformation on my input features to make them easier to model, then use a simple model (e.g. linear regression) that instead uses the transformed features.\"\n",
    "Doing this means your total model is $y = ABx + b$ where $B$ is the matrix multiplying an input point $x$ into a new representation and $A$ is the matrix parameterizing the linear regression.\n",
    "\n",
    "But, $AB$ is just another matrix, and so by adding a representation you have not made your model more powerful; instead if you'd \"twisted\" the input space after appyling B, the overall map would be nonlinear and the composite model would have greater representation power.\n",
    "Activation functions perform this \"twisting\".\n",
    "Deep neural networks come from the observation that it'd be easier to get a good representation (top layer) if it was based on a lower-level representation (early layers).\n",
    "\n",
    "Here's a great article explaining the geometric interpretation of activation functions: https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/.\n",
    "The general idea is that neural networks can learn parameters that use the \"twists\" such that the entire network deforms space so that the manifold defined by your input data is simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function with gradient descent\n",
    "Minimize the scalar function $f(x) = (x-1)(2x-2)(x-3)(x-4)$, plotted below, using gradient descent.\n",
    "It has a local minimum near $x = 1$ and a global minimum near $x = 3.5$.\n",
    "\n",
    "![f(x)](./images/plot_f.png)\n",
    "\n",
    "The steps to build the graph are:\n",
    " 1. Use `tf.Variable` to create a variable named `x` that uses a `np.random.uniform` on the range [-1, 5] to initialize.\n",
    " 2. Make a `tf.optimizers.SGD` named \"optimizer\" with a learning rate of 0.01.\n",
    " 3. Make a function `optimize` that takes in an optimizer as an argument and represents each step of the training loop.\n",
    " 4. Create a `tf.GradientTape` object and make a tensor `y` that represents $f(x)$ given a value of `x` under it.\n",
    " 5. Get the gradients of `y` from the `tf.GradientTape` and apply them to the optimizer.\n",
    " \n",
    "Remember steps 4 and 5 go inside the `optimize` function!\n",
    "The whole subgraph for this problem should go under a name scope of \"minimize_f\", and operations to compute `y` should have an additional name scope of \"compute_f\". \n",
    "\n",
    "In a comment, rewrite the `optimize` function using the `minimize` function on the optimizer instead of getting the gradients and applying them. Is `tf.GradientTape` is necessary? You do not need to worry about `tf.name_scope` for this.\n",
    "\n",
    "Then, the steps to minimize the function once are:\n",
    " 1. Print the initial values of `x` and `y`.\n",
    " 2. Run `minimize` 1000 times.\n",
    " 3. Print the final values of `x` and `y`.\n",
    " \n",
    "Minimize the function a few times. If you did it right, you'll find that in each run the optimizer finds one of two minima. Running minimization a few times, you should see it find both eventually. What determines which minimum is found? Answer in the markdown box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph code here\n",
    "iterations = 1000\n",
    "\n",
    "def f(x): # graph\n",
    "    return (x - 1) * (2 * x - 2) * (x - 3) * (x - 4)\n",
    "\n",
    "def optimize(optimizer):\n",
    "    with tf.name_scope('minimize_f'):\n",
    "        x = tf.Variable(np.random.uniform(-1, 5), dtype=tf.float32, name='x')\n",
    "\n",
    "        for step in range(iterations):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # watch variable with tape\n",
    "                tape.watch(x)\n",
    "                with tf.name_scope('compute_f'):\n",
    "                    y = f(x)\n",
    "\n",
    "            # compute gradient\n",
    "            gradients = tape.gradient(y, x)\n",
    "\n",
    "            # apply gradient\n",
    "            optimizer.apply_gradients([(gradients, x)])\n",
    "\n",
    "        return x, f(x)\n",
    "\n",
    "\n",
    "# if we were to write this without tf.GradientTape(), we can simply put the code \n",
    "# below in the name_scope\n",
    "# x = tf.Variable(np.random.uniform(-1, 5), dtype=tf.float32, name='x')\n",
    "# optimizer.minimize(lambda: f(x), var_list=[x])\n",
    "#the minimize function automatically computes the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Final x: 3.5930705070495605, Final f(x): -3.245518922805786\n",
      "Run 2 - Final x: 0.9999998807907104, Final f(x): 1.7053025658242404e-13\n",
      "Run 3 - Final x: 0.9999998807907104, Final f(x): 1.7053025658242404e-13\n",
      "Run 4 - Final x: 3.5930705070495605, Final f(x): -3.245518922805786\n",
      "Run 5 - Final x: 1.000000238418579, Final f(x): 6.821209179094789e-13\n"
     ]
    }
   ],
   "source": [
    "# Your training loop here\n",
    "\n",
    "for i in range(5):  # see the diff values returned\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "    x_final, y_final = optimize(optimizer)\n",
    "    print(f\"Run {i + 1} - Final x: {x_final.numpy()}, Final f(x): {y_final.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum that is reached is determined by the randomly initialized x that we start with. Depending on the values in this tensor, we can descend into either of the two local minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
